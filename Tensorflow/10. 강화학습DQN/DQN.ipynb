{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN개념\n",
    "- DQN은 Deep Q-network의 줄임말.\n",
    "- 강화학습 알고리즘으로 유명한 Q-learning을 딥러닝으로 구현했다는 말\n",
    "\n",
    "강화학습이란 어떤 환경에서 인공지능 에이전트가 현재상태(환경)을 판단하여 가장 이로운 행동을 하게 만드는 학습 방법이다.\n",
    "- 이로운행동 -> 보상을 줌\n",
    "- 해로운행동 -> 패널티를 줌\n",
    "- 즉 누적된 이득이 최대가 되게 행동하도록 학습이 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q-learning\n",
    ": 어떤 상태에서 특정행동을 했을때 가치를 나타내는 함수인 Q함수를 학습하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 Q러닝을 신경망으로 구현하면 학습이 상당히 불안해 질 수 있다. 이는 아래 두 방법을 사용하여 해결\n",
    "1. 먼저 과거의 상태를 기억한 뒤 그중 임의의 상태를 뽑아 학습 \n",
    "    1. 이렇게하면 특수한 상황에 치우치지않도록 조절할 수 있다.\n",
    "2. 손실값을 계산하기위해 학습을 진행하면서 최적의 행동을 얻어내는 기본신경망과 얻어낸 값이 좋은 선택인지 비교하는 목표신경망을 분리하는 방법을 사용한다.\n",
    "    1. 목표신경망은 계속 갱신하는 것이 아니라 기본신경망의 학습된 결괏값을 일정주기마다 목표신경망에 갱신한다.\n",
    "   \n",
    "그리고 DQN은 화면의 상태, 즉 화면영상만으로 게임을 학습한다. 따라서 이미지 인식에 뛰어난 CNN을 사용하여 신경망 모델을 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 에이전트 구현하기\n",
    "- OpenAI라는 비영리회사에서 제공하는 Gym(https://gym.openai.com/) 이라는 강화학습 알고리즘 개발도구가 있음. \n",
    "- 책 저자가 만들어놓은 간단한 게임 game.py를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에이전트 : 게임의 상태를 입력받아 신경망으로 전달, 신경망에서 판단한 행동을 게임에 적용해서 다음단계로 진행\n",
    "- 에이전트는 train모드와 게임실행모드(replay)로 나뉜다. \n",
    "    - 학습 모드일때는 게임을 화면에 보여주지 않은 채 빠르게 실행하여 학습속도를 높이고\n",
    "    - 게임실행모드에서는 학습한 결과를 이용하여 게임을 진행하면서 화면에 출력도 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from game import *\n",
    "from model import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 실행시 모드를 나누어 실행할 수 있도록 tf.app.flags를 이용해 실행시 받을 옵션들을 설정한다.\n",
    "tf.app.flags.DEFINE_boolean(\"train\",False,\"학습모드. 게임을 화면에 보여주지 않습니다.\")\n",
    "FLAGS=tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE=10000 # 최대로 학습할 게임 횟수\n",
    "TARGET_UPDATE_INTERVAL=1000 #학습을 일정횟수만큼 진행할 떄 마다 한번씩 목표신경망을 갱신하라는 옵션\n",
    "TRAIN_INTERVAL=4 # 게임 4프레임(상태)마다 한번씩 학습하라는 얘기\n",
    "OBSERVE=100 #일정수준의 학습데이터가 쌓이기 전까지는 학습하지 않고 지켜보기만 하라는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 게임에서 취할 수 있는 행동은 좌,우,상태유지 이렇게 세개이다(떨어지는 물건을 좌우로 피하는 게임)\n",
    "# 게임화면은 가로6칸, 세로10칸으로 설정하였다.\n",
    "NUM_ACTION=3 # 행동 -0:좌,1:유지,2:우\n",
    "SCREEN_WIDTH=6\n",
    "SCREEN_HEIGHT=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('뇌세포 깨우는 중..')\n",
    "    sess = tf.Session()\n",
    "\n",
    "    game = Game(SCREEN_WIDTH, SCREEN_HEIGHT, show_game=False)\n",
    "    brain = DQN(sess, SCREEN_WIDTH, SCREEN_HEIGHT, NUM_ACTION)\n",
    "\n",
    "    rewards = tf.placeholder(tf.float32, [None])\n",
    "    tf.summary.scalar('avg.reward/ep.', tf.reduce_mean(rewards))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    summary_merged = tf.summary.merge_all()\n",
    "\n",
    "    # 타겟 네트웍을 초기화합니다. # 목표신경망을 한번 초기돠 해준다. 학습된 결과가 없으므로 여기서 목표신경망값은 초기화된 기본 신경망 값과 같다.\n",
    "    brain.update_target_network()\n",
    "\n",
    "    # 다음에 취할 액션을 DQN 을 이용해 결정할 시기를 결정합니다.\n",
    "    epsilon = 1.0\n",
    "    # 프레임 횟수\n",
    "    time_step = 0\n",
    "    total_reward_list = []\n",
    "\n",
    "    # 게임을 시작합니다.\n",
    "    for episode in range(MAX_EPISODE):\n",
    "        terminal = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # 게임을 초기화하고 현재 상태를 가져옵니다.\n",
    "        # 상태는 screen_width x screen_height 크기의 화면 구성입니다.\n",
    "        state = game.reset()\n",
    "        brain.init_state(state)\n",
    "\n",
    "        while not terminal:\n",
    "            # 입실론이 랜덤값보다 작은 경우에는 랜덤한 액션을 선택하고\n",
    "            # 그 이상일 경우에는 DQN을 이용해 액션을 선택합니다.\n",
    "            # 초반엔 학습이 적게 되어 있기 때문입니다.\n",
    "            # 초반에는 거의 대부분 랜덤값을 사용하다가 점점 줄어들어\n",
    "            # 나중에는 거의 사용하지 않게됩니다.\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.randrange(NUM_ACTION)\n",
    "            else:\n",
    "                action = brain.get_action()\n",
    "\n",
    "            # 일정 시간이 지난 뒤 부터 입실론 값을 줄입니다.\n",
    "            # 초반에는 학습이 전혀 안되어 있기 때문입니다.\n",
    "            if episode > OBSERVE:\n",
    "                epsilon -= 1 / 1000\n",
    "\n",
    "            # 결정한 액션을 이용해 게임을 진행하고, 보상과 게임의 종료 여부를 받아옵니다.\n",
    "            state, reward, terminal = game.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # 현재 상태를 Brain에 기억시킵니다.\n",
    "            # 기억한 상태를 이용해 학습하고, 다음 상태에서 취할 행동을 결정합니다.\n",
    "            brain.remember(state, action, reward, terminal)\n",
    "\n",
    "            if time_step > OBSERVE and time_step % TRAIN_INTERVAL == 0:\n",
    "                # DQN 으로 학습을 진행합니다.\n",
    "                brain.train()\n",
    "\n",
    "            if time_step % TARGET_UPDATE_INTERVAL == 0:\n",
    "                # 타겟 네트웍을 업데이트 해 줍니다.\n",
    "                brain.update_target_network()\n",
    "\n",
    "            time_step += 1\n",
    "\n",
    "        print('게임횟수: %d 점수: %d' % (episode + 1, total_reward))\n",
    "\n",
    "        total_reward_list.append(total_reward)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            summary = sess.run(summary_merged, feed_dict={rewards: total_reward_list})\n",
    "            writer.add_summary(summary, time_step)\n",
    "            total_reward_list = []\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, 'model/dqn.ckpt', global_step=time_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay():\n",
    "    print('뇌세포 깨우는 중..')\n",
    "    sess = tf.Session()\n",
    "\n",
    "    game = Game(SCREEN_WIDTH, SCREEN_HEIGHT, show_game=True)\n",
    "    brain = DQN(sess, SCREEN_WIDTH, SCREEN_HEIGHT, NUM_ACTION)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state('model')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    # 게임을 시작합니다.\n",
    "    for episode in range(MAX_EPISODE):\n",
    "        terminal = False\n",
    "        total_reward = 0\n",
    "\n",
    "        state = game.reset()\n",
    "        brain.init_state(state)\n",
    "\n",
    "        while not terminal:\n",
    "            action = brain.get_action()\n",
    "\n",
    "            # 결정한 액션을 이용해 게임을 진행하고, 보상과 게임의 종료 여부를 받아옵니다.\n",
    "            state, reward, terminal = game.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            brain.remember(state, action, reward, terminal)\n",
    "\n",
    "            # 게임 진행을 인간이 인지할 수 있는 속도로^^; 보여줍니다.\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        print('게임횟수: %d 점수: %d' % (episode + 1, total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if FLAGS.train:\n",
    "        train()\n",
    "    else:\n",
    "        replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
